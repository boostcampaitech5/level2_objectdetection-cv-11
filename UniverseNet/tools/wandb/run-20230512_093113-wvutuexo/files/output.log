
2023-05-12 09:32:39,341 - mmdet - INFO - Epoch [1][50/977]	lr: 1.766e-06, eta: 8:12:47, time: 1.517, data_time: 0.058, memory: 16485, stage0_loss_cls: 2.2257, stage0_pos_acc: 11.3130, stage0_loss_bbox: 5.2950, stage0_loss_iou: 1.8651, stage1_loss_cls: 2.3154, stage1_pos_acc: 10.6398, stage1_loss_bbox: 6.0037, stage1_loss_iou: 2.0100, stage2_loss_cls: 2.3166, stage2_pos_acc: 11.3428, stage2_loss_bbox: 4.4143, stage2_loss_iou: 2.1158, stage3_loss_cls: 2.1873, stage3_pos_acc: 12.1265, stage3_loss_bbox: 5.7094, stage3_loss_iou: 2.4731, stage4_loss_cls: 2.2717, stage4_pos_acc: 9.2084, stage4_loss_bbox: 6.8670, stage4_loss_iou: 2.5996, stage5_loss_cls: 1.9641, stage5_pos_acc: 21.9810, stage5_loss_bbox: 7.0073, stage5_loss_iou: 2.6787, loss: 62.3198, grad_norm: 36485.4201
2023-05-12 09:33:52,757 - mmdet - INFO - Epoch [1][100/977]	lr: 3.550e-06, eta: 8:03:38, time: 1.468, data_time: 0.009, memory: 16485, stage0_loss_cls: 2.1835, stage0_pos_acc: 11.6558, stage0_loss_bbox: 4.1630, stage0_loss_iou: 1.7453, stage1_loss_cls: 2.2806, stage1_pos_acc: 9.6929, stage1_loss_bbox: 4.2248, stage1_loss_iou: 1.8395, stage2_loss_cls: 2.2821, stage2_pos_acc: 11.1091, stage2_loss_bbox: 2.5296, stage2_loss_iou: 1.7778, stage3_loss_cls: 2.1332, stage3_pos_acc: 11.7010, stage3_loss_bbox: 2.6190, stage3_loss_iou: 1.9594, stage4_loss_cls: 2.2216, stage4_pos_acc: 9.8023, stage4_loss_bbox: 2.6678, stage4_loss_iou: 1.9255, stage5_loss_cls: 1.8252, stage5_pos_acc: 24.1563, stage5_loss_bbox: 2.5028, stage5_loss_iou: 2.0084, loss: 42.8889, grad_norm: 21722.7500
2023-05-12 09:35:06,166 - mmdet - INFO - Epoch [1][150/977]	lr: 5.334e-06, eta: 7:59:44, time: 1.468, data_time: 0.008, memory: 16485, stage0_loss_cls: 2.0906, stage0_pos_acc: 14.1460, stage0_loss_bbox: 2.4258, stage0_loss_iou: 1.4947, stage1_loss_cls: 2.1575, stage1_pos_acc: 11.7028, stage1_loss_bbox: 1.9719, stage1_loss_iou: 1.5003, stage2_loss_cls: 2.1263, stage2_pos_acc: 14.4193, stage2_loss_bbox: 1.5689, stage2_loss_iou: 1.5449, stage3_loss_cls: 1.8925, stage3_pos_acc: 22.0388, stage3_loss_bbox: 1.6304, stage3_loss_iou: 1.6189, stage4_loss_cls: 1.9587, stage4_pos_acc: 17.4767, stage4_loss_bbox: 1.5630, stage4_loss_iou: 1.5973, stage5_loss_cls: 1.6990, stage5_pos_acc: 27.4123, stage5_loss_bbox: 1.5621, stage5_loss_iou: 1.6046, loss: 32.0074, grad_norm: 10397.9608
2023-05-12 09:36:20,145 - mmdet - INFO - Epoch [1][200/977]	lr: 7.118e-06, eta: 7:58:06, time: 1.480, data_time: 0.008, memory: 16485, stage0_loss_cls: 1.9458, stage0_pos_acc: 17.7731, stage0_loss_bbox: 1.5467, stage0_loss_iou: 1.3294, stage1_loss_cls: 1.8327, stage1_pos_acc: 22.0767, stage1_loss_bbox: 1.3622, stage1_loss_iou: 1.3817, stage2_loss_cls: 1.8380, stage2_pos_acc: 24.1980, stage2_loss_bbox: 1.4298, stage2_loss_iou: 1.4934, stage3_loss_cls: 1.6693, stage3_pos_acc: 26.1672, stage3_loss_bbox: 1.3983, stage3_loss_iou: 1.4785, stage4_loss_cls: 1.6436, stage4_pos_acc: 23.7560, stage4_loss_bbox: 1.4056, stage4_loss_iou: 1.4619, stage5_loss_cls: 1.6443, stage5_pos_acc: 24.4517, stage5_loss_bbox: 1.3622, stage5_loss_iou: 1.4272, loss: 27.6506, grad_norm: 3623.0612
2023-05-12 09:37:33,919 - mmdet - INFO - Epoch [1][250/977]	lr: 8.902e-06, eta: 7:56:22, time: 1.475, data_time: 0.009, memory: 16485, stage0_loss_cls: 1.7515, stage0_pos_acc: 24.3907, stage0_loss_bbox: 1.3139, stage0_loss_iou: 1.3008, stage1_loss_cls: 1.6608, stage1_pos_acc: 29.5972, stage1_loss_bbox: 1.2582, stage1_loss_iou: 1.3480, stage2_loss_cls: 1.6023, stage2_pos_acc: 29.8448, stage2_loss_bbox: 1.2942, stage2_loss_iou: 1.3964, stage3_loss_cls: 1.5959, stage3_pos_acc: 27.8626, stage3_loss_bbox: 1.3428, stage3_loss_iou: 1.4080, stage4_loss_cls: 1.5141, stage4_pos_acc: 28.1715, stage4_loss_bbox: 1.3700, stage4_loss_iou: 1.4487, stage5_loss_cls: 1.5153, stage5_pos_acc: 29.2793, stage5_loss_bbox: 1.3837, stage5_loss_iou: 1.4737, loss: 25.9785, grad_norm: 1384.1746
2023-05-12 09:38:48,388 - mmdet - INFO - Epoch [1][300/977]	lr: 1.069e-05, eta: 7:55:32, time: 1.489, data_time: 0.009, memory: 16485, stage0_loss_cls: 1.6261, stage0_pos_acc: 29.0399, stage0_loss_bbox: 1.2397, stage0_loss_iou: 1.2478, stage1_loss_cls: 1.5666, stage1_pos_acc: 33.8910, stage1_loss_bbox: 1.2349, stage1_loss_iou: 1.2770, stage2_loss_cls: 1.5180, stage2_pos_acc: 33.2084, stage2_loss_bbox: 1.2707, stage2_loss_iou: 1.3304, stage3_loss_cls: 1.5121, stage3_pos_acc: 29.6532, stage3_loss_bbox: 1.3104, stage3_loss_iou: 1.3530, stage4_loss_cls: 1.4526, stage4_pos_acc: 30.3002, stage4_loss_bbox: 1.3053, stage4_loss_iou: 1.3615, stage5_loss_cls: 1.4690, stage5_pos_acc: 30.5701, stage5_loss_bbox: 1.4179, stage5_loss_iou: 1.3978, loss: 24.8904, grad_norm: 990.1777
2023-05-12 09:40:02,360 - mmdet - INFO - Epoch [1][350/977]	lr: 1.247e-05, eta: 7:54:08, time: 1.479, data_time: 0.009, memory: 16485, stage0_loss_cls: 1.5440, stage0_pos_acc: 32.8108, stage0_loss_bbox: 1.2117, stage0_loss_iou: 1.2779, stage1_loss_cls: 1.5942, stage1_pos_acc: 27.1766, stage1_loss_bbox: 1.2056, stage1_loss_iou: 1.3133, stage2_loss_cls: 1.5017, stage2_pos_acc: 26.0516, stage2_loss_bbox: 1.1612, stage2_loss_iou: 1.2901, stage3_loss_cls: 1.4734, stage3_pos_acc: 27.1567, stage3_loss_bbox: 1.2767, stage3_loss_iou: 1.3373, stage4_loss_cls: 1.4565, stage4_pos_acc: 23.3977, stage4_loss_bbox: 1.2845, stage4_loss_iou: 1.3634, stage5_loss_cls: 1.4270, stage5_pos_acc: 25.1815, stage5_loss_bbox: 1.3767, stage5_loss_iou: 1.4326, loss: 24.5279, grad_norm: 637.7651
2023-05-12 09:41:16,601 - mmdet - INFO - Epoch [1][400/977]	lr: 1.425e-05, eta: 7:53:00, time: 1.485, data_time: 0.009, memory: 16485, stage0_loss_cls: 1.5467, stage0_pos_acc: 37.8724, stage0_loss_bbox: 1.2095, stage0_loss_iou: 1.2372, stage1_loss_cls: 1.5298, stage1_pos_acc: 27.6357, stage1_loss_bbox: 1.1467, stage1_loss_iou: 1.2193, stage2_loss_cls: 1.4825, stage2_pos_acc: 29.0705, stage2_loss_bbox: 1.1851, stage2_loss_iou: 1.2477, stage3_loss_cls: 1.4601, stage3_pos_acc: 25.2787, stage3_loss_bbox: 1.2046, stage3_loss_iou: 1.2716, stage4_loss_cls: 1.4105, stage4_pos_acc: 25.0701, stage4_loss_bbox: 1.2904, stage4_loss_iou: 1.3285, stage5_loss_cls: 1.4232, stage5_pos_acc: 29.0178, stage5_loss_bbox: 1.3368, stage5_loss_iou: 1.3787, loss: 23.9090, grad_norm: 423.7715
2023-05-12 09:42:30,792 - mmdet - INFO - Epoch [1][450/977]	lr: 1.604e-05, eta: 7:51:48, time: 1.484, data_time: 0.009, memory: 16485, stage0_loss_cls: 1.5629, stage0_pos_acc: 29.0795, stage0_loss_bbox: 1.2562, stage0_loss_iou: 1.2705, stage1_loss_cls: 1.5263, stage1_pos_acc: 26.7747, stage1_loss_bbox: 1.2473, stage1_loss_iou: 1.2895, stage2_loss_cls: 1.4742, stage2_pos_acc: 27.1525, stage2_loss_bbox: 1.2327, stage2_loss_iou: 1.2768, stage3_loss_cls: 1.4076, stage3_pos_acc: 26.5972, stage3_loss_bbox: 1.2601, stage3_loss_iou: 1.3151, stage4_loss_cls: 1.4052, stage4_pos_acc: 23.4068, stage4_loss_bbox: 1.2960, stage4_loss_iou: 1.3476, stage5_loss_cls: 1.3991, stage5_pos_acc: 27.1670, stage5_loss_bbox: 1.4360, stage5_loss_iou: 1.4453, loss: 24.4485, grad_norm: 379.2773
Traceback (most recent call last):
  File "main.py", line 183, in <module>
    train(cfg)
  File "main.py", line 118, in train
    train_detector(model, datasets, cfg, distributed=False, validate=True,meta=meta)
  File "/opt/ml/level2_objectdetection-cv-11/UniverseNet/mmdet/apis/train.py", line 306, in train_detector
    runner.run(data_loaders, cfg.workflow)
  File "/opt/conda/envs/project2/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py", line 136, in run
    epoch_runner(data_loaders[i], **kwargs)
  File "/opt/conda/envs/project2/lib/python3.7/site-packages/mmcv/runner/epoch_based_runner.py", line 54, in train
    self.call_hook('after_train_iter')
  File "/opt/conda/envs/project2/lib/python3.7/site-packages/mmcv/runner/base_runner.py", line 317, in call_hook
    getattr(hook, fn_name)(self)
  File "/opt/conda/envs/project2/lib/python3.7/site-packages/mmcv/runner/hooks/optimizer.py", line 68, in after_train_iter
    grad_norm = self.clip_grads(runner.model.parameters())
  File "/opt/conda/envs/project2/lib/python3.7/site-packages/mmcv/runner/hooks/optimizer.py", line 59, in clip_grads
    return clip_grad.clip_grad_norm_(params, **self.grad_clip)
  File "/opt/conda/envs/project2/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py", line 42, in clip_grad_norm_
    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
  File "/opt/conda/envs/project2/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py", line 42, in <listcomp>
    total_norm = torch.norm(torch.stack([torch.norm(p.grad.detach(), norm_type).to(device) for p in parameters]), norm_type)
  File "/opt/conda/envs/project2/lib/python3.7/site-packages/torch/functional.py", line 1312, in norm
    return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore[attr-defined]
KeyboardInterrupt